{"cells":[{"cell_type":"markdown","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"326e97c3-897d-4f23-adb3-475796ac1561","inputWidgets":{},"title":"Flux rentrant et flux sortant avec Spark Streaming"}}},{"cell_type":"markdown","source":["1. Obtenir la clé API sur Alpha Vantage pour accéder aux données financières.\n\n2. Créer un nouveau répertoire dans le compte GitHub pour héberger le projet Spark-trading.\n\n3. Créer un nouveau notebook dans DataBricks pour exécuter le code Spark-trading.\n\n4. Importer les bibliothèques nécessaires dans ce notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22318f54-49da-4148-8128-9da6bda55a55","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\n\nimport requests"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"68a071b1-c5a8-4065-95ff-066251d2a465","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["5. Créer une instance du StreamingContext avec la configuration Spark appropriée"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7e947e89-537a-4879-860f-6bbae56f15e0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Créer un objet SparkContext\nsc = SparkContext(appName=\"StreamingExample\")\n\n# Créer un objet StreamingContext avec un intervalle de traitement spécifié (par exemple, 1 seconde)\n#ssc = StreamingContext(sparkContext=sc, batchDuration=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5d0f6070-32c7-4778-b0bc-f6717fcd3465","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-577569076373841>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Créer un objet SparkContext\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mStreamingExample\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m     )\n\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    205\u001B[0m         master,\n\u001B[1;32m    206\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    217\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    493\u001B[0m             currentAppName,\n\u001B[1;32m    494\u001B[0m             currentMaster,\n\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    498\u001B[0m         )\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","File \u001B[0;32m<command-577569076373841>:2\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Créer un objet SparkContext\u001B[39;00m\n","\u001B[0;32m----> 2\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mStreamingExample\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n","\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n","\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m    200\u001B[0m     )\n","\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n","\u001B[1;32m    205\u001B[0m         master,\n","\u001B[1;32m    206\u001B[0m         appName,\n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n","\u001B[1;32m    217\u001B[0m     )\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n","\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n","\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n","\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n","\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n","\u001B[1;32m    493\u001B[0m             currentAppName,\n","\u001B[1;32m    494\u001B[0m             currentMaster,\n","\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n","\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n","\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n","\u001B[1;32m    498\u001B[0m         )\n","\u001B[1;32m    499\u001B[0m     )\n","\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n","\n","\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "]}}],"execution_count":0},{"cell_type":"markdown","source":["6. Définir la source de données en utilisant la méthode socketTextStream pour un flux rentrant. Par ex, nous utiliserons la méthode foreachRDD pour télécharger les données d'Alpha Vantage à partir de l'API."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5e676f39-4265-44e8-b274-7a425745019d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["ssc = StreamingContext(sparkContext=sc, batchDuration=1)\n# Définir la source de données avec le flux rentrant\nlines = ssc.socketTextStream(\"localhost\", 9999)\n# Appliquez des transformations sur le flux rentrant\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n# Définir la fonction pour télécharger les données d'Alpha Vantage à partir de l'API\n\ndef download_data(rdd):\n    # Clé API Alpha Vantage\n    api_key = \"<JZ3M5S1Y55LBSNSH>\"\n\n    # URL de l'API Alpha Vantage pour récupérer les données\n    url = 'https://www.alphavantage.co/query\"qfunction=ALL_COMMODITIES&interval=monthly&apikey=JZ3M5S1Y55LBSNSH'\n    r = requests.get(url)\n    data = r.json()\n\nprint(data)\n# Boucle sur chaque enregistrement du RDD\nfor record in rdd.collect():\n    # Récupération du symbole de la société à partir de l'enregistrement\n    symbol = record.strip()\n    # Paramètres de la requête\n    params = {\n        \"function\": \"TIME_SERIES_INTRADAY\",\n        \"symbol\": symbol,\n        \"interval\": \"1min\",\n        \"apikey\": api_key\n    }\n\n# Envoi de la requête GET à l'API Alpha Vantage\nresponse = requests.get(url, params=params)\n\n# Votre code pour traiter la réponse et enregistrer les données dans Spark\ndf = (spark\n     .readStream\n     .option(\"maxFilesPerTrigger\", 1)\n     .format(\"json\")\n     .load(events_path)\n     )\n \ndf.isStreaming\n\n# Appliquez la fonction de téléchargement de données à chaque RDD du flux rentrant\n#lines.foreachRDD(download_data)\n# Appliquez la fonction de téléchargement de données à chaque RDD du flux rentrant\nlines.foreachRDD(lambda rdd: rdd.foreach(download_data))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22b1a2d4-98f9-4a96-9c74-1b0c3499bfad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-577569076373844>\", line 17, in <module>\n    for record in rdd.collect():\nNameError: name 'rdd' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'rdd' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["7. Appliquer les opérations de transformation ou d'analyse sur les données téléchargées."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"295f75cb-dc56-43c5-9c8d-e342b60e4fbc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["8. Enregistrer les résultats dans des fichiers."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"31d81b31-1889-46ae-ac67-2db197351844","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["9. Démarrer le traitement du flux en utilisant la méthode start du StreamingContext"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5405c529-84df-4838-8372-eae1a8feec62","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["ssc.start()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4fcc7cc8-6b19-40f6-b40e-8cd96f9c41ec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["10. Attendez que le traitement soit terminé ou définissez des conditions d'arrêt, puis arrêtez le contexte de streaming :"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"24cb1183-6d8f-452d-932f-60da43e06a4c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["ssc.awaitTermination()\nssc.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5ee4c2f5-730b-4fdc-9189-8255541fdfd9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["11. Dans GitHub, créer un fichier .txt contenant la clé API d'Alpha Vantage, puis enregistrer-le dans le répertoire approprié.\n\n12. Dans DataBricks, utiliser la méthode read.text() pour lire le fichier contenant la clé API à partir de GitHub."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"86e1455c-6dc1-4611-8b95-c6e9ffc904b5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Lire le fichier contenant la clé API depuis GitHub\napi_key = spark.read.text(\"https://github.com/orion-alpha-kplr/Spark-trading/api.txt\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"77b0de90-56c5-4330-81d4-006a3fa8590c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark-trading","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
